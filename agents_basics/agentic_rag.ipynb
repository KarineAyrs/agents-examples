{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cb5c9f-24b3-4d92-820f-8dfb6445a798",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "## –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ RAG\n",
    "\n",
    "- –û–¥–∏–Ω —à–∞–≥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n",
    "- –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ä–∏–∑–æ–Ω–∏–Ω–≥ - –æ–±—ã—á–Ω—ã–µ RAG –ø–∞–π–ø–ª–∞–π–Ω—ã –Ω–µ –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–æ–≤–µ—Ä—à–∞—Ç—å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–π —Ä–∏–∑–æ–Ω–∏–Ω–≥ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è\n",
    "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–æ —à–∏—Ä–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ - –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–æ–ª–∂–Ω—ã –ø–æ–¥—Ö–æ–¥–∏—Ç—å –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º—É –æ–∫–Ω—É –º–æ–¥–µ–ª–∏\n",
    "\n",
    "## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Agentic RAG \n",
    "\n",
    "- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∑–∞–ø—Ä–æ—Å—ã, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\n",
    "- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ: –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∏–∑–≤–ª–µ–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ\n",
    "- –†–∏–∑–æ–Ω–∏–Ω–≥ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã\n",
    "- –°–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞ –∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ: –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏ —É–ª—É—á—à–∏—Ç—å —Å–≤–æ–π –ø–æ—Ö–æ–¥\n",
    "\n",
    "## –î–∞–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –æ–±—Ä–∞–∑–æ–º —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ RAG —Ç–µ—Ö–Ω–∏–∫–∏: \n",
    "- HyDE (Hypothetical Document Embedding ): –≤–º–µ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞–ø—Ä—è–º—É—é, –∞–≥–µ–Ω—Ç —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã, –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è \n",
    "- Self-Query Refinement: –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å —É—Ç–æ—á–Ω–µ–Ω–Ω—ã–º–∏ –∑–∞–ø—Ä–æ—Å–∞–º–∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3122ac12-4021-48fb-a161-432d8122a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install rank_bm25\n",
    "# !pip install langchain-ollama\n",
    "# !pip install langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d98fc-e409-4a60-9ca8-89db9d0c0fbc",
   "metadata": {},
   "source": [
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537c2e4b-df6b-4cfc-b94d-5ebce9e6eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karineayrapetyants/projects/agent-lectures/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.tools import BaseTool, tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebdd53e-1051-4ace-ad5b-ad7605c888f3",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π \n",
    "\n",
    "- –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é HuggingFace\n",
    "- –í–æ–∑—å–º–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–Ω–æ—Å—è—â—É—é—Å—è –∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbbdd6f-8342-4c7b-942e-ccb9f85c6342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base prepared with 14695 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Load the Hugging Face documentation dataset\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "# Filter to include only Transformers documentation\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "# Convert dataset entries to Document objects with metadata\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "# Split documents into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Characters per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Priority order for splitting\n",
    ")\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "\n",
    "print(f\"Knowledge base prepared with {len(docs_processed)} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18045cd-3d11-472b-9cc0-29f5120a6e1c",
   "metadata": {},
   "source": [
    "## Tool –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101d7623-b332-41b7-b267-b4b0885c78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieverInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\")\n",
    "    \n",
    "class RetrieverTool(BaseTool):\n",
    "    args_schema: Type[BaseModel] = RetrieverInput\n",
    "    return_direct: bool = True\n",
    "    retriever: Type[BM25Retriever] = None\n",
    "\n",
    "    name: str = \"retriever\"\n",
    "    description: str = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the retriever with our processed documents\n",
    "        self.retriever = BM25Retriever.from_documents(\n",
    "            docs, k=10  # Return top 10 most relevant documents\n",
    "        )\n",
    "\n",
    "    def _run(self, query: str) -> str:\n",
    "\n",
    "        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.invoke(query)\n",
    "\n",
    "        \n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b27cd-34af-46d6-86d9-72e614634d07",
   "metadata": {},
   "source": [
    "### –û–ø—Ä–µ–¥–µ–ª–∏–º toolkit –∞–≥–µ–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1bcca16-fbfd-4cfb-a25a-0562820d8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = RetrieverTool(docs_processed)\n",
    "toolkit = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d146ad5c-ea34-4cfd-b99f-13ed5ca0bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant.\n",
    "    Use your tools to answer questions. If you do not have a tool to\n",
    "    answer the question, say so. \n",
    "    Return only the answers.\n",
    "    \"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae69335-5825-43ad-a131-43812d422016",
   "metadata": {},
   "source": [
    "## –ê–≥–µ–Ω—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87303beb-43a8-4180-a071-8f71d9060b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zs/mv0lpz1n46q3ynw23qw42btc0000gn/T/ipykernel_92874/858769194.py:2: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  agent = create_react_agent(model, toolkit, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "model = ChatOllama(model=\"gpt-oss:20b-cloud\")\n",
    "agent = create_react_agent(model, toolkit, prompt=prompt)\n",
    "\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"For a transformers model training, which is slower, the forward or the backward pass?\"}\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a198ae46-9912-4d9a-9314-ff483a76af4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "For a transformers model training, which is slower, the forward or the backward pass?\n",
      "None\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retriever (881599fc-4212-40c4-87ed-5e7bc729cb66)\n",
      " Call ID: 881599fc-4212-40c4-87ed-5e7bc729cb66\n",
      "  Args:\n",
      "    query: transformers training forward pass slower than backward pass\n",
      "None\n",
      "\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retriever\n",
      "\n",
      "\n",
      "Retrieved documents:\n",
      "\n",
      "\n",
      "===== Document 0 =====\n",
      "Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in \n",
      "significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed \n",
      "during the backward pass, would introduce a considerable computational overhead and slow down the training process.\n",
      "\n",
      "===== Document 1 =====\n",
      "- A train step function which combines the loss function and optimizer update, does the forward and backward pass and returns the updated parameters.\n",
      "\n",
      "===== Document 2 =====\n",
      "For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates \n",
      "into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually \n",
      "bandwidth-limited, and it‚Äôs typical for an activation to have to read more data in the backward than in the forward \n",
      "(e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\n",
      "\n",
      "===== Document 3 =====\n",
      "model_inputs = tokenizer(src_text, text_target=tgt_text, return_tensors=\"pt\")\n",
      "\n",
      "model(**model_inputs)  # forward pass\n",
      "```\n",
      "\n",
      "- Generation\n",
      "\n",
      "===== Document 4 =====\n",
      "```python\n",
      "from transformers import Tool\n",
      "\n",
      "\n",
      "class HFModelDownloadsTool(Tool):\n",
      "    pass\n",
      "```\n",
      "\n",
      "===== Document 5 =====\n",
      "## How to benchmark ü§ó Transformers models\n",
      "\n",
      "The classes [`PyTorchBenchmark`] and [`TensorFlowBenchmark`] allow to flexibly benchmark ü§ó Transformers models. The benchmark classes allow us to measure the _peak memory usage_ and _required time_ for both _inference_ and _training_.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "Hereby, _inference_ is defined by a single forward pass, and _training_ is defined by a single forward pass and\n",
      "backward pass.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "===== Document 6 =====\n",
      "layer is falsely activated during the forward pass, *i.e.* pass *self.training* to [PyTorch's functional dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout)\n",
      "\n",
      "===== Document 7 =====\n",
      "The best way to fix the problem is usually to look at the forward pass of the original implementation and the ü§ó\n",
      "Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out\n",
      "intermediate outputs of both implementations of the forward pass to find the exact position in the network where the ü§ó\n",
      "Transformers implementation shows a different output than the original implementation. First, make sure that the\n",
      "\n",
      "===== Document 8 =====\n",
      "The best way to fix the problem is usually to look at the forward pass\n",
      "of the original implementation and the ü§ó Transformers implementation\n",
      "side-by-side and check if there are any differences. Ideally, you should\n",
      "debug/print out intermediate outputs of both implementations of the\n",
      "forward pass to find the exact position in the network where the ü§ó\n",
      "Transformers implementation shows a different output than the original\n",
      "implementation. First, make sure that the hard-coded `input_ids` in both\n",
      "\n",
      "===== Document 9 =====\n",
      "The best way to fix the problem is usually to look at the forward pass\n",
      "of the original implementation and the ü§ó Transformers implementation\n",
      "side-by-side and check if there are any differences. Ideally, you should\n",
      "debug/print out intermediate outputs of both implementations of the\n",
      "forward pass to find the exact position in the network where the ü§ó\n",
      "Transformers implementation shows a different output than the original\n",
      "implementation. First, make sure that the hard-coded `input_ids` in both\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in response[\"messages\"]:\n",
    "    print(message.pretty_print())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
